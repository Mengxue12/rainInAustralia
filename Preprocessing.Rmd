---
title: "Preprocessing"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(dplyr)
library(ggplot2)
library(kableExtra)
library(gridExtra)
library(stats)
library(cluster)
library(factoextra)
library(FactoMineR)
library(mclust)
library(caret)
```


In the following chunk of code a tiny data pre processing will be applied to the dataset in order to prepare it execute few clustering algorithms on top of it. To apply the clustering algorithms below the input dataset must be composed by numeric variables, therefore not numeric data will be represented with the one hot encoding format.
The analysis will be performed considering climatic descriptors, discarding the city and the number of the sampling.
```{r}
# Need caret library

# To have reproducibility of the sampling
set.seed(123)

scaled <- read.csv('scaled.csv')

# Removing the first two column describing the number of the row and the city
scaled <- scaled[3:ncol(scaled)]

# Preparing data for MBC
# Keeping just numeric values
df.mbc <- scaled %>% dplyr::select(where(is.numeric))
# The column RainToday RainTomorrow and Season
x <- scaled[16:18]
dmy <- dummyVars(" ~ .", data = x)
trsf <- data.frame(predict(dmy, newdata = x))
df.mbc <- cbind(df.mbc,trsf)
df.mbc <-df.mbc[sample(nrow(df.mbc),20000), ]

# Preparing data for HC
dmy <- dummyVars(" ~ .", data = scaled)
trsf <- data.frame(predict(dmy, newdata = scaled))

# Random sampling to make the clustering computation fit in my machine

df.pc <- trsf[sample(nrow(trsf),20000), ]
```

The first approach with clustering method have been with the traditional partition methodology applying K-Means algorithm, since is the computationally less expensive technique. The algorithm have been executed, looking for 2, 3, 4 and 5 clusters (`centers = x`) in order to look for some likely shapes of the clusters.
```{r}
summary(df.pc)
df.pc

k2 <- kmeans(df.pc, centers = 2, nstart = 25)
k3 <- kmeans(df.pc, centers = 3, nstart = 25)
k4 <- kmeans(df.pc, centers = 4, nstart = 25)
k5 <- kmeans(df.pc, centers = 5, nstart = 25)

p1 <- fviz_cluster(k2, geom = "point", data = df.pc) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point",  data = df.pc) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point",  data = df.pc) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point",  data = df.pc) + ggtitle("k = 5")
grid.arrange(p1, p2, p3, p4, nrow = 2)
```

To determine the optimal number of clusters the silhouette method have been adopted, with the respective code `method = "silhouette"`. The output suggest an optimal number of clusters equal to two.
```{r}
fviz_nbclust(df.pc, kmeans, method = "silhouette")
```

As the silhouette method suggested will be studied the clustering with k equals to 2. For the interpretation of the obtained results, showing the centers `k2$centers` will help to associate each cluster to particular feature.
The cluster number 1 seems describing the records with warmer environments and the cluster number 2 the record with lower temperature since `MinTemp` and `MaxTemp` are way higher in the first cluster than in the second one. This deduction is actually right because the first cluster represent the most of the summer season and the second cluster represents the most of the winter season (`Seasonsummer` and `Seasonwinter`).

```{r}
k2$centers[,c('MinTemp','MaxTemp','RainTodayNo','RainTodayYes','RainTomorrowNo','RainTomorrowYes','Seasonfall','Seasonspring','Seasonsummer','Seasonwinter')]
```

Another trial to identify other kind of clusters have been a mixed approach, using a hierarchical clustering to determine the number of clusters. The number of clusters then have been used to set up the `k` parameter in K-Means algorithm (`hkmeans`).
```{r}
res.hk <-hkmeans(df.pc, 4)
fviz_cluster(res.hk, palette = "jco", repel = TRUE, ggtheme = theme_classic())
```
Cluster 4 represents mostly the rainy day `RainTodayYes`. The feature of a rainy day then seems being that there is a probability of rain the day after (`RainTomorrowYes`) higher then non rainy days. We can also notice that the rainy days are at most in summer and fall.
```{r}
res.hk$centers
```


Since the biggest part of the dataset shows a gaussian distribution, a Gaussian finite mixture model fitted by EM algorithm should achieve good results in terms of clustering.
```{r}
summary(df.mbc)
df.mbc

mc <- Mclust(df.mbc)
mc2 <- Mclust(df.mbc,2)
mc4 <- Mclust(df.mbc,4)

fviz_mclust(mc2, "uncertainty", palette = "jco")
fviz_mclust(mc4, "uncertainty", palette = "jco")

```
```{r}
mc$G
```

```{r}
mc2$parameters$mean
```



```{r}
mc4$parameters$mean
```